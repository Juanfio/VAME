"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1967],{5676:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>d});var t=i(4848),o=i(8453);const a={title:"Running VAME Workflow",sidebar_position:2},r=void 0,s={id:"getting_started/running",title:"Running VAME Workflow",description:"Workflow Overview",source:"@site/docs/getting_started/running.mdx",sourceDirName:"getting_started",slug:"/getting_started/running",permalink:"/VAME/docs/getting_started/running",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{title:"Running VAME Workflow",sidebar_position:2},sidebar:"docsSidebar",previous:{title:"Installation",permalink:"/VAME/docs/getting_started/installation"},next:{title:"API reference",permalink:"/VAME/docs/category/api-reference"}},l={},d=[{value:"Workflow Overview",id:"workflow-overview",level:2},{value:"Running a demo workflow",id:"running-a-demo-workflow",level:2},{value:"1. Download the necessary resources:",id:"1-download-the-necessary-resources",level:3},{value:"2. Running the demo main pipeline",id:"2-running-the-demo-main-pipeline",level:3},{value:"2.1a Setting the demo variables using CSV files",id:"21a-setting-the-demo-variables-using-csv-files",level:4},{value:"2.1b Setting the demo variables using NWB files",id:"21b-setting-the-demo-variables-using-nwb-files",level:4},{value:"2.2 Initializing the project",id:"22-initializing-the-project",level:4},{value:"2.3 Egocentric alignment",id:"23-egocentric-alignment",level:4},{value:"2.4 Creating the training dataset",id:"24-creating-the-training-dataset",level:4},{value:"2.5 Training the model",id:"25-training-the-model",level:4},{value:"2.6 Evaluate the model",id:"26-evaluate-the-model",level:4},{value:"2.7 Segmenting the behavior",id:"27-segmenting-the-behavior",level:4},{value:"3. Running Optional Steps of the Pipeline",id:"3-running-optional-steps-of-the-pipeline",level:3},{value:"3.1 Creating motif videos",id:"31-creating-motif-videos",level:4},{value:"3.2 Run community detection",id:"32-run-community-detection",level:4},{value:"3.3 Community Videos",id:"33-community-videos",level:4},{value:"3.4 UMAP Visualization",id:"34-umap-visualization",level:4},{value:"3.5 Generative Model (Reconstruction decoder)",id:"35-generative-model-reconstruction-decoder",level:4},{value:"3.6 Create output video",id:"36-create-output-video",level:4}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"workflow-overview",children:"Workflow Overview"}),"\n",(0,t.jsxs)(n.p,{children:["The below diagram shows the workflow of the VAME application, which consists of four main steps and optional steps to analyse your data.\n",(0,t.jsx)(n.img,{alt:"Workflow Overview",src:i(2416).A+"",width:"753",height:"92"})]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Initialize project: This is step is responsible by starting the project, getting your data into the right format and creating a training dataset for the VAME deep learning model."}),"\n",(0,t.jsx)(n.li,{children:"Train neural network: Train a variational autoencoder which is parameterized with recurrent neural network to embed behavioural dynamics"}),"\n",(0,t.jsx)(n.li,{children:"Evaluate performance: Evaluate the trained model based on its reconstruction capabilities"}),"\n",(0,t.jsx)(n.li,{children:"Segment behavior: Segment behavioural motifs/poses/states from the input time series"}),"\n",(0,t.jsxs)(n.li,{children:["Quantify behavior:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Optional: Create motif videos to get insights about the fine grained poses."}),"\n",(0,t.jsx)(n.li,{children:"Optional: Investigate the hierarchical order of your behavioural states by detecting communities in the resulting markov chain."}),"\n",(0,t.jsx)(n.li,{children:"Optional: Create community videos to get more insights about behaviour on a hierarchical scale."}),"\n",(0,t.jsx)(n.li,{children:"Optional: Visualization and projection of latent vectors onto a 2D plane via UMAP."}),"\n",(0,t.jsx)(n.li,{children:"Optional: Use the generative model (reconstruction decoder) to sample from the learned data distribution, reconstruct random real samples or visualize the cluster centre for validation."}),"\n",(0,t.jsx)(n.li,{children:"Optional: Create a video of an egocentrically aligned animal + path through the community space (similar to our gif on github readme)."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["\u26a0\ufe0f Check out also the published VAME Workflow Guide, including more hands-on recommendations and tricks ",(0,t.jsx)(n.a,{href:"https://www.nature.com/articles/s42003-022-04080-7#Sec8",children:"HERE"}),"."]})}),"\n",(0,t.jsx)(n.h2,{id:"running-a-demo-workflow",children:"Running a demo workflow"}),"\n",(0,t.jsxs)(n.p,{children:["In our github in ",(0,t.jsx)(n.code,{children:"/examples"})," folder there is a demo script called ",(0,t.jsx)(n.code,{children:"demo.py"})," that you can use to run a simple example of the VAME workflow. To run this workflow you will need to do the following:"]}),"\n",(0,t.jsx)(n.h3,{id:"1-download-the-necessary-resources",children:"1. Download the necessary resources:"}),"\n",(0,t.jsx)(n.p,{children:"To run the demo you will need a video and a csv file with the pose estimation results. You can use the following files links:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"video-1.mp4"}),": Video file ",(0,t.jsx)(n.a,{href:"https://drive.google.com/file/d/1w6OW9cN_-S30B7rOANvSaR9c3O5KeF0c/view",children:"link"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"video-1.csv"}),": Pose estimation results ",(0,t.jsx)(n.a,{href:"https://github.com/EthoML/VAME/blob/master/examples/video-1.csv",children:"link"})]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-running-the-demo-main-pipeline",children:"2. Running the demo main pipeline"}),"\n",(0,t.jsx)(n.p,{children:"We will now show you how to run the main pipeline of the VAME workflow using  snnipets of code. We suggest you to run these snippets in a jupyter notebook."}),"\n",(0,t.jsx)(n.h4,{id:"21a-setting-the-demo-variables-using-csv-files",children:"2.1a Setting the demo variables using CSV files"}),"\n",(0,t.jsx)(n.p,{children:"To start the demo you must define 4 variables:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import vame\n\n\n# The directory where the project will be saved\nworking_directory = '.'\n\n# The name you want for the project\nproject = 'my-vame-project'\n\n# A list of paths to the videos file\nvideos = ['video-1.mp4']\n\n# A list of paths to the poses estimations files.\n# Important: The name (without the extension) of the video file and the pose estimation file must be the same. E.g. `video-1.mp4` and `video-1.csv`\nposes_estimations = ['video-1.csv']\n"})}),"\n",(0,t.jsx)(n.h4,{id:"21b-setting-the-demo-variables-using-nwb-files",children:"2.1b Setting the demo variables using NWB files"}),"\n",(0,t.jsxs)(n.p,{children:["Alternativaly you can use ",(0,t.jsx)(n.code,{children:".nwb"})," files as pose estimation files. In this case you must define 4 variables:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import vame\n\n\n# The directory where the project will be saved\nworking_directory = '.'\n\n# The name you want for the project\nproject = 'my-vame-project'\n\n# A list of paths to the videos file\nvideos = ['video-1.mp4']\n\n# A list of paths to the poses estimations files.\n# Important: The name (without the extension) of the video file and the pose estimation file must be the same. E.g. `video-1.mp4` and `video-1.nwb`\nposes_estimations = ['video-1.nwb']\n\n# A list of paths in the NWB file where the pose estimation data is stored.\npaths_to_pose_nwb_series_data = ['processing/behavior/data_interfaces/PoseEstimation/pose_estimation_series']\n"})}),"\n",(0,t.jsx)(n.h4,{id:"22-initializing-the-project",children:"2.2 Initializing the project"}),"\n",(0,t.jsx)(n.p,{children:"With the variables set, you can initialize the project by running the following code:"}),"\n",(0,t.jsx)(n.p,{children:"If you are using CSV files you can run the following code to initialize the project:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"config = vame.init_new_project(\n    project=project,\n    videos=videos,\n    poses_estimations=poses_estimations,\n    working_directory=working_directory,\n    videotype='.mp4'\n)\n"})}),"\n",(0,t.jsx)(n.p,{children:"If you are using NWB files you can run the following code to initialize the project:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"config = vame.init_new_project(\n    project=project,\n    videos=videos,\n    poses_estimations=poses_estimations,\n    working_directory=working_directory,\n    videotype='.mp4',\n    paths_to_pose_nwb_series_data=paths_to_pose_nwb_series_data\n)\n"})}),"\n",(0,t.jsxs)(n.p,{children:["This command will create a project folder in the defined working directory with the name you set in the ",(0,t.jsx)(n.code,{children:"project"})," variable and a date suffix, e.g: ",(0,t.jsx)(n.code,{children:"my-vame-project-May-9-2024"}),".\nIn this folder you can find a config file called ",(0,t.jsx)(n.code,{children:"config.yaml"})," where you can set the parameters for the VAME algorithm.\nThe videos and poses estimations files will be copied to the project videos folder. It is really important to define in the ",(0,t.jsx)(n.code,{children:"config.yaml"})," file if your data is egocentrically aligned or not before running the rest of the workflow."]}),"\n",(0,t.jsx)(n.h4,{id:"23-egocentric-alignment",children:"2.3 Egocentric alignment"}),"\n",(0,t.jsx)(n.p,{children:"If your data is not egocentrically aligned, you can align it by running the following code:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.egocentric_alignment(config, pose_ref_index=[0, 5])\n"})}),"\n",(0,t.jsx)(n.p,{children:"But if your experiment is by design egocentrical (e.g. head-fixed experiment on treadmill etc) you can use the following to convert your .csv to a .npy array, ready to train vame on it."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.csv_to_numpy(config)\n"})}),"\n",(0,t.jsx)(n.h4,{id:"24-creating-the-training-dataset",children:"2.4 Creating the training dataset"}),"\n",(0,t.jsx)(n.p,{children:"To create the training dataset you can run the following code:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.create_trainset(config, pose_ref_index=[0,5])\n"})}),"\n",(0,t.jsx)(n.h4,{id:"25-training-the-model",children:"2.5 Training the model"}),"\n",(0,t.jsx)(n.p,{children:"Training the vame model might take a while depending on the size of your dataset and your machine settings. To train the model you can run the following code:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.train_model(config)\n"})}),"\n",(0,t.jsx)(n.h4,{id:"26-evaluate-the-model",children:"2.6 Evaluate the model"}),"\n",(0,t.jsx)(n.p,{children:"THe model evaluation produces two plots, one showing the loss of the model during training and the other showing the reconstruction and future prediction of input sequence."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.evaluate_model(config)\n"})}),"\n",(0,t.jsx)(n.h4,{id:"27-segmenting-the-behavior",children:"2.7 Segmenting the behavior"}),"\n",(0,t.jsx)(n.p,{children:"To perform pose segmentation you can run the following code:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.pose_segmentation(config)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"3-running-optional-steps-of-the-pipeline",children:"3. Running Optional Steps of the Pipeline"}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"The following steps are optional and can be run if you want to create motif VideoColorSpace, communities/hierarchies of behavior and community VideoColorSpace."})}),"\n",(0,t.jsx)(n.h4,{id:"31-creating-motif-videos",children:"3.1 Creating motif videos"}),"\n",(0,t.jsx)(n.p,{children:"To create motif videos and get insights about the fine grained poses you can run:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.motif_videos(config, videoType='.mp4')\n"})}),"\n",(0,t.jsx)(n.h4,{id:"32-run-community-detection",children:"3.2 Run community detection"}),"\n",(0,t.jsx)(n.p,{children:"To create behavioral hierarchies and communities detection run:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.community(config, parametrization='hmm', cut_tree=2, cohort=False)\n"})}),"\n",(0,t.jsx)(n.p,{children:"It will produce a tree plot of the behavioural hierarchies using hmm motifs."}),"\n",(0,t.jsx)(n.h4,{id:"33-community-videos",children:"3.3 Community Videos"}),"\n",(0,t.jsx)(n.p,{children:"Create community videos to get insights about behavior on a hierarchical scale."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.community_videos(config)\n"})}),"\n",(0,t.jsx)(n.h4,{id:"34-umap-visualization",children:"3.4 UMAP Visualization"}),"\n",(0,t.jsx)(n.p,{children:"Down projection of latent vectors and visualization via UMAP."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'fig = vame.visualization(config, label=None) #options: label: None, "motif", "community"\n'})}),"\n",(0,t.jsx)(n.h4,{id:"35-generative-model-reconstruction-decoder",children:"3.5 Generative Model (Reconstruction decoder)"}),"\n",(0,t.jsx)(n.p,{children:"Use the generative model (reconstruction decoder) to sample from the learned data distribution, reconstruct random real samples or visualize\nthe cluster center for validation."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'vame.generative_model(config, mode="centers") #options: mode: "sampling", "reconstruction", "centers", "motifs"\n'})}),"\n",(0,t.jsx)(n.h4,{id:"36-create-output-video",children:"3.6 Create output video"}),"\n",(0,t.jsx)(n.p,{children:"Create a video of an egocentrically aligned mouse + path through\nthe community space (similar to our gif on github) to learn more about your representation\nand have something cool to show around."}),"\n",(0,t.jsx)(n.admonition,{type:"warning",children:(0,t.jsx)(n.p,{children:"This function is currently very slow."})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.gif(config, pose_ref_index=[0,5], subtract_background=True, start=None,\n         length=500, max_lag=30, label='community', file_format='.mp4', crop_size=(300,300))\n"})}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"Once the frames are saved you can create a video or gif via e.g. ImageJ or other tools"})})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},2416:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/workflow_overview-eaa7ffc1b70952df84277ded89585b52.png"},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>s});var t=i(6540);const o={},a=t.createContext(o);function r(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);